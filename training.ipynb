{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac2d9340-5c89-4e54-84f7-424c348f414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from dataset import MyDataset\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import csv\n",
    "from unet import UNet, UNetL, UNetLL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c737dc34-d9d2-43e2-a6a9-3361b0462cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_score(preds, targets, threshold=0.5, eps=1e-6):\n",
    "    preds = torch.sigmoid(preds)\n",
    "    preds = (preds > threshold).float()\n",
    "    intersection = (preds * targets).sum(dim=(1, 2, 3))\n",
    "    union = preds.sum(dim=(1, 2, 3)) + targets.sum(dim=(1, 2, 3)) - intersection\n",
    "    return ((intersection + eps) / (union + eps)).mean()\n",
    "\n",
    "def precision_score(preds, targets, threshold=0.5, eps=1e-6):\n",
    "    preds = torch.sigmoid(preds)\n",
    "    preds = (preds > threshold).float()\n",
    "    true_positive = (preds * targets).sum()\n",
    "    predicted_positive = preds.sum()\n",
    "    return (true_positive + eps) / (predicted_positive + eps)\n",
    "\n",
    "def recall_score(preds, targets, threshold=0.5, eps=1e-6):\n",
    "    preds = torch.sigmoid(preds)\n",
    "    preds = (preds > threshold).float()\n",
    "    true_positive = (preds * targets).sum()\n",
    "    actual_positive = targets.sum()\n",
    "    return (true_positive + eps) / (actual_positive + eps)\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1.0):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = torch.sigmoid(logits)\n",
    "\n",
    "        probs = probs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        intersection = (probs * targets).sum()\n",
    "        dice = (2. * intersection + self.smooth) / (\n",
    "            probs.sum() + targets.sum() + self.smooth\n",
    "        )\n",
    "\n",
    "        return 1 - dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a44c17d-a4d5-46e3-8ee8-031c6fc42583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "\n",
      "Used a fraction of 1.0 GPU's memory\n",
      "Weighted BCE:\tFalse\n",
      "Tiles Dimension: 256x256\n",
      "Training dataset dimension: 56000\n",
      "Validation dataset dimension: 4000\n",
      "1750 batches of 32 images\n",
      "\n",
      "GPU ID: 0\n",
      "GPU Total Memory: 47.53 GB\n"
     ]
    }
   ],
   "source": [
    "starting_time = time.time()\n",
    "\n",
    "architecture_name = 'unetLL'\n",
    "model_dataset = 'IAD'\n",
    "dataset_kind = 'InriaAerialDataset'\n",
    "tile_dimension = 256\n",
    "batch_size = 32\n",
    "weightedBCE = False\n",
    "diceLoss = True\n",
    "earlystop = False\n",
    "\n",
    "# Initial setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# device = 'cpu'\n",
    "print(f\"Device: {device}\\n\")\n",
    "memory_fraction = 1.\n",
    "print(f\"Used a fraction of {memory_fraction} GPU's memory\")\n",
    "print(f\"Weighted BCE:\\t{weightedBCE}\")\n",
    "\n",
    "training_dataset_portion = 1\n",
    "validation_dataset_portion = 0.5\n",
    "num_epochs = 30\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "patience = 5\n",
    "early_stop_counter = 0\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset_path = '/mnt/nas151/sar/Footprint/datasets/'\n",
    "\n",
    "\n",
    "# Dataset and DataLoader\n",
    "train_dataset_full = MyDataset(image_dir=dataset_path + dataset_kind + f'/tiles_{tile_dimension}/train/images',\n",
    "                          mask_dir=dataset_path + dataset_kind + f'/tiles_{tile_dimension}/train/gt',\n",
    "                          transform=transform)\n",
    "\n",
    "# In order to train with fewer images, this part will mix and select a portion (dataset_portion) of the entire training dataset.\n",
    "num_samples = len(train_dataset_full)\n",
    "subset_size = int(training_dataset_portion * num_samples)\n",
    "indices = np.random.permutation(num_samples)[:subset_size]\n",
    "train_dataset = Subset(train_dataset_full, indices)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset_full = MyDataset(image_dir=dataset_path + dataset_kind + f'/tiles_{tile_dimension}/val/images',\n",
    "                          mask_dir=dataset_path + dataset_kind + f'/tiles_{tile_dimension}/val/gt',\n",
    "                          transform=transform)\n",
    "\n",
    "# In order to train with fewer images, this part will mix and select a portion (dataset_portion) of the entire training dataset.\n",
    "num_samples = len(val_dataset_full)\n",
    "subset_size = int(validation_dataset_portion * num_samples)\n",
    "indices = np.random.permutation(num_samples)[:subset_size]\n",
    "val_dataset = Subset(val_dataset_full, indices)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "tot_batches = int(len(train_dataset)/batch_size)\n",
    "\n",
    "print(f\"Tiles Dimension: {tile_dimension}x{tile_dimension}\")\n",
    "\n",
    "print(f'Training dataset dimension: {len(train_dataset)}')\n",
    "print(f'Validation dataset dimension: {len(val_dataset)}')\n",
    "\n",
    "print(f\"{tot_batches} batches of {batch_size} images\")\n",
    "\n",
    "if weightedBCE:\n",
    "    '''start = time.time()\n",
    "    print('Computing the weights...')\n",
    "    tile_length = len(train_loader.dataset[0][1][0]) * len(train_loader.dataset[0][1][0][0])\n",
    "    pos_freq = 0\n",
    "    neg_freq = 0\n",
    "    for tile in tqdm(train_loader.dataset):\n",
    "        pos_freq += torch.sum(tile[1][0])\n",
    "\n",
    "    neg_freq = tile_length * len(train_loader.dataset) - pos_freq\n",
    "    pos_weight = neg_freq / (tile_length * len(train_loader.dataset))\n",
    "    neg_weight = pos_freq / (tile_length * len(train_loader.dataset))\n",
    "\n",
    "    print(f'Positive weight:\\t{pos_weight}\\nNegative weight:\\t{neg_weight}')\n",
    "    weight = torch.tensor([pos_weight, neg_weight])\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight.to(device), size_average=None, reduce=None, reduction='mean')'''\n",
    "    loss_name = 'WBCE'\n",
    "    # First attempt. I need to change for the frequency of buildings and background pixels\n",
    "    start = time.time()\n",
    "    print('Computing the weights...')\n",
    "    tile_length = len(train_loader.dataset[0][1][0]) * len(train_loader.dataset[0][1][0][0])\n",
    "    pos_freq = 0\n",
    "    neg_freq = 0\n",
    "    for tile in tqdm(train_loader.dataset):\n",
    "        pos_freq += torch.sum(tile[1][0])\n",
    "\n",
    "    neg_freq = tile_length * len(train_loader.dataset) - pos_freq\n",
    "    weight = torch.tensor(neg_freq / pos_freq)\n",
    "    print(f'Positive weight:\\t{weight:.2f}')\n",
    "    bce_criterion = nn.BCEWithLogitsLoss(pos_weight=weight)\n",
    "else:\n",
    "    loss_name = 'BCE'\n",
    "    bce_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "if diceLoss:\n",
    "    loss_name += 'plusDL'\n",
    "    dice_criterion = DiceLoss()\n",
    "else:\n",
    "    dice_criterion = None\n",
    "# model initialization, loss and optimizer\n",
    "if architecture_name == 'unet':\n",
    "    model = UNet(n_channels=3, n_classes=1).to(device)\n",
    "elif architecture_name == 'unetL':\n",
    "    model = UNetL(n_channels=3, n_classes=1).to(device)\n",
    "elif architecture_name == 'unetLL':\n",
    "    model = UNetLL(n_channels=3, n_classes=1).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.01, total_iters=20)\n",
    "\n",
    "model_name = f'{architecture_name}_{model_dataset}_{loss_name}_n{len(train_dataset)}_dim{tile_dimension}x{tile_dimension}_bs{batch_size}'\n",
    "os.makedirs(f'/home/antoniocorvino/Projects/BuildingsExtraction/runs/{model_name}', exist_ok=True)\n",
    "\n",
    "# Saving metrics\n",
    "csv_metrics = f\"/home/antoniocorvino/Projects/BuildingsExtraction/runs/{model_name}/metrics.csv\"\n",
    "\n",
    "with open(csv_metrics, mode='w', newline='') as f:\n",
    "    writer_csv = csv.writer(f)\n",
    "    writer_csv.writerow([\"epoch\",\n",
    "                         \"train_epoch_loss\", \"train_iou\", \"train_precision\", \"train_recall\",\n",
    "                         \"val_epoch_loss\", \"val_iou\", \"val_precision\", \"val_recall\",\n",
    "                         \"time\"])\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_id = torch.cuda.current_device()\n",
    "    print(f\"\\nGPU ID: {gpu_id}\")\n",
    "    print(f\"GPU Total Memory: {torch.cuda.get_device_properties(gpu_id).total_memory/1024**3:.2f} GB\")\n",
    "    torch.cuda.set_per_process_memory_fraction(memory_fraction, device=gpu_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f06ce19d-13f2-413e-aa62-a562533bc588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH ---- 1/30\n",
      "Current Learning Rate: 0.001000\n",
      "\n",
      "Training is started...\n",
      "\n",
      "Memory Allocated: 0.00 GB\n",
      "Memory Reserved: 0.02 GB\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'criterion' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dice_criterion:\n\u001b[0;32m---> 36\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m(outputs, masks)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     loss \u001b[38;5;241m=\u001b[39m bce_criterion(outputs, masks)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'criterion' is not defined"
     ]
    }
   ],
   "source": [
    "best_loss = 1e4\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"EPOCH ---- {epoch+1}/{num_epochs}\")\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f'Current Learning Rate: {current_lr:.6f}')\n",
    "    print(\"\\nTraining is started...\\n\")\n",
    "    print(f\"Memory Allocated: {torch.cuda.memory_allocated(gpu_id)/1024**3:.2f} GB\")\n",
    "    print(f\"Memory Reserved: {torch.cuda.memory_reserved(gpu_id)/1024**3:.2f} GB\")\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "\n",
    "    epoch_train_loss = 0.0\n",
    "    batch_number = 0\n",
    "    iou_total = 0.0\n",
    "    prec_total = 0.0\n",
    "    recall_total = 0.0\n",
    "\n",
    "    for images, masks in train_loader:\n",
    "        \n",
    "        batch_number += 1\n",
    "\n",
    "        #print(f\"\\nBatch #{batch_number}\")\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device).float()  # This must be \"float\" for the loss function\n",
    "\n",
    "        #print(f\"Memory allocated by the batch: {torch.cuda.memory_allocated(gpu_id)/1024**2:.2f} MB\")\n",
    "        #print(f\"Memory reserved by the batch: {torch.cuda.memory_reserved(gpu_id)/1024**2:.2f} MB\")\n",
    "        #batch_memory = images.element_size() * images.nelement() + masks.element_size() * masks.nelement()\n",
    "        #print(f\"Memory occupied by the batch (images and masks only): {batch_memory / 1024 ** 2:.2f} MB\")\n",
    "\n",
    "        outputs = model(images)\n",
    "\n",
    "        if dice_criterion:\n",
    "            loss = bce_criterion(outputs, masks)\n",
    "        else:\n",
    "            loss = bce_criterion(outputs, masks)\n",
    "        with torch.no_grad():\n",
    "            iou_batch = iou_score(outputs, masks)\n",
    "            prec_batch = precision_score(outputs, masks)\n",
    "            recall_batch = recall_score(outputs, masks)\n",
    "\n",
    "        iou_total += iou_batch\n",
    "        prec_total += prec_batch\n",
    "        recall_total += recall_batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        if batch_number % int(0.1 * tot_batches) == 0:\n",
    "            print(f\"\\rProgress: {(100 * batch_number/tot_batches):.0f}% -- time: {int(elapsed_time//60):02d}:{int(elapsed_time%60):02d}\", end=\"\")\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    epoch_train_loss /= len(train_loader)\n",
    "\n",
    "    train_iou = (iou_total / len(train_loader)).item()\n",
    "    train_prec = (prec_total / len(train_loader)).item()\n",
    "    train_recall = (recall_total / len(train_loader)).item()\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"\\nValidation is started...\")\n",
    "\n",
    "    # --- VALIDATION STEP ---\n",
    "    starting_val = time.time()\n",
    "    model.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    val_iou = 0.0\n",
    "    val_prec = 0.0\n",
    "    val_recall = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_images, val_masks in val_loader:\n",
    "            val_images = val_images.to(device)\n",
    "            val_masks = val_masks.to(device).float()\n",
    "\n",
    "            val_outputs = model(val_images)\n",
    "            loss_val = criterion(val_outputs, val_masks)\n",
    "            epoch_val_loss += loss_val.item()\n",
    "\n",
    "            val_iou += iou_score(val_outputs, val_masks)\n",
    "            val_prec += precision_score(val_outputs, val_masks)\n",
    "            val_recall += recall_score(val_outputs, val_masks)\n",
    "\n",
    "    # Average validation metrics\n",
    "    epoch_val_loss /= len(val_loader)\n",
    "    val_iou = (val_iou / len(val_loader)).item()\n",
    "    val_prec = (val_prec / len(val_loader)).item()\n",
    "    val_recall = (val_recall / len(val_loader)).item()\n",
    "\n",
    "    if epoch_val_loss < best_loss:\n",
    "        best_loss = epoch_val_loss\n",
    "        print(best_loss, epoch)\n",
    "        torch.save(model.state_dict(), f\"/home/antoniocorvino/Projects/BuildingsExtraction/runs/{model_name}/best_model.pth\")\n",
    "\n",
    "    \n",
    "    print(f\"\\nTime for Validation: {int(time.time() - starting_val):02d}s\\n\")\n",
    "\n",
    "    print(f'\\nTRAINING\\tLoss: {epoch_train_loss:.4f}\\tIoU: {train_iou:.4f}\\tPrecision: {train_prec:.4f}\\tRecall: {train_recall:.4f}')\n",
    "    print(f\"VALIDATION\\tLoss: {epoch_val_loss:.4f}\\tIoU: {val_iou:.4f}\\tPrecision: {val_prec:.4f}\\tRecall: {val_recall:.4f}\\n\\n\")\n",
    "\n",
    "    # Save metrics to CSV\n",
    "    with open(csv_metrics, mode='a', newline='') as f:\n",
    "        writer_csv = csv.writer(f)\n",
    "        writer_csv.writerow([epoch + 1,\n",
    "                             epoch_train_loss, train_iou, train_prec, train_recall,\n",
    "                             epoch_val_loss, val_iou, val_prec, val_recall,\n",
    "                             round(elapsed_time, 2)])\n",
    "    scheduler.step()\n",
    "    # Checkpoint\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Checkpoint {epoch+1}\")\n",
    "        torch.save(model.state_dict(), f\"/home/antoniocorvino/Projects/BuildingsExtraction/runs/{model_name}/checkpoint_{epoch+1}.pth\")\n",
    "    if earlystop:\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            counter = 0\n",
    "\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            print(f\"No improvement for {early_stop_counter} epoch(s).\")\n",
    "\n",
    "        if early_stop_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            torch.save(model.state_dict(), f\"/home/antoniocorvino/Projects/BuildingsExtraction/runs/{model_name}/best_model.pth\")\n",
    "\n",
    "            break\n",
    "print(f'Total time: {((time.time() - starting_time)/60):.2f} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95eceb7c-b129-4d81-b593-9e67e494e989",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
